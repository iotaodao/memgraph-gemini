print("üöÄ [1/6] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Python...")
import os
import json
import uuid
import time
import re
import glob
import sys
# –î–û–ë–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç —Ç–∏–ø–æ–≤
from typing import List, Dict, Any

# –õ–æ–≤–∏–º –æ—à–∏–±–∫–∏ –∏–º–ø–æ—Ä—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
try:
    from dotenv import load_dotenv
    load_dotenv()
    import google.generativeai as genai
    from chonkie import TokenChunker
    from neo4j import GraphDatabase
    print("‚úÖ [2/6] –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    sys.exit(1)

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
MEMGRAPH_URI = os.getenv("MEMGRAPH_URI", "bolt://localhost:7687")
MEMGRAPH_AUTH = (os.getenv("MEMGRAPH_USER", ""), os.getenv("MEMGRAPH_PASSWORD", ""))

if "GEMINI_API_KEY" not in os.environ:
    print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω GEMINI_API_KEY")
    sys.exit(1)

genai.configure(api_key=os.environ["GEMINI_API_KEY"])

class HybridGraphPipeline:
    def __init__(self, uri, auth, extraction_model="gemini-2.5-flash"):
        print(f"üîå [3/6] –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Memgraph ({uri})...")
        try:
            self.driver = GraphDatabase.driver(uri, auth=auth)
            self.driver.verify_connectivity()
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ!")
        except Exception as e:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ: {e}")
            sys.exit(1)
            
        self.chunker = TokenChunker(tokenizer="gpt2", chunk_size=512, chunk_overlap=50)
        self.extraction_model = genai.GenerativeModel(
            model_name=extraction_model,
            system_instruction="Extract entities (Person, Org, Tech) and relationships (SCREAMING_SNAKE_CASE). JSON output: {entities: [], relations: []}.",
            generation_config={"temperature": 0.1, "response_mime_type": "application/json"}
        )
        self.embedding_model_name = "models/text-embedding-004" 

    def close(self):
        self.driver.close()

    def _generate_embedding(self, text: str) -> List[float]:
        try:
            return genai.embed_content(
                model=self.embedding_model_name,
                content=text,
                task_type="retrieval_document"
            )['embedding']
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∞: {e}")
            return []

    def _extract_graph_data(self, text: str) -> Dict[str, Any]:
        try:
            resp = self.extraction_model.generate_content(f"Extract graph from:\\n\\n{text}")
            return json.loads(resp.text)
        except:
            return {"entities": [], "relations": []}

    def process_directory(self, data_dir: str):
        abs_path = os.path.abspath(data_dir)
        print(f"üìÇ [4/6] –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏: {abs_path}")
        
        if not os.path.exists(data_dir):
            print(f"‚ùå –ü–∞–ø–∫–∞ '{data_dir}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            return

        files = glob.glob(os.path.join(data_dir, "**/*.txt"), recursive=True) + \
                glob.glob(os.path.join(data_dir, "**/*.md"), recursive=True)
        
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
        if len(files) == 0:
            print("‚ö†Ô∏è –ü–∞–ø–∫–∞ –ø—É—Å—Ç–∞ –∏–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –∏–º–µ—é—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è .txt/.md")
            return
        
        print("‚ñ∂Ô∏è [5/6] –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
        with self.driver.session() as session:
            for filepath in files:
                filename = os.path.basename(filepath)
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ID
                doc_id = re.sub(r'[^a-zA-Z0-9_-]', '_', filename)
                print(f"   üî™ –ß–∏—Ç–∞—é —Ñ–∞–π–ª: {filename}")
                
                try:
                    with open(filepath, "r", encoding="utf-8") as f: 
                        text = f.read().replace('\\0', '')
                    
                    if not text.strip():
                        print("   ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞—é.")
                        continue

                    chunks = self.chunker(text)
                    print(f"      üß© –ß–∞–Ω–∫–æ–≤: {len(chunks)}")

                    # 1. –î–æ–∫—É–º–µ–Ω—Ç (JSON DUMPS)
                    session.run(f"MERGE (d:Document {{id: {json.dumps(doc_id)}}})")

                    for i, chunk in enumerate(chunks):
                        graph_data = self._extract_graph_data(chunk.text)
                        vector = self._generate_embedding(chunk.text)
                        chunk_id = str(uuid.uuid4())

                        # 2. –ß–∞–Ω–∫ (JSON DUMPS)
                        query_chunk = f"""
                        MATCH (d:Document {{id: {json.dumps(doc_id)}}})
                        MERGE (c:Chunk {{id: {json.dumps(chunk_id)}}})
                        SET c.index = {i}
                        SET c.text = {json.dumps(chunk.text)}
                        SET c.embedding = {json.dumps(vector)}
                        MERGE (d)-[:HAS_CHUNK]->(c)
                        """
                        session.run(query_chunk)

                        # 3. –°—É—â–Ω–æ—Å—Ç–∏
                        for ent in graph_data.get("entities", []):
                            e_id = ent.get("id", "").strip()
                            e_type = ent.get("type", "Thing").strip()
                            if not e_id: continue
                            
                            e_type = re.sub(r'[^a-zA-Z0-9_]', '', e_type) or "Thing"
                            
                            q_ent = f"""
                            MATCH (c:Chunk {{id: {json.dumps(chunk_id)}}}) 
                            MERGE (e:Entity {{id: {json.dumps(e_id)}}}) 
                            ON CREATE SET e.type = {json.dumps(e_type)}
                            MERGE (c)-[:MENTIONS]->(e)
                            """
                            session.run(q_ent)

                        # 4. –°–≤—è–∑–∏
                        for rel in graph_data.get("relations", []):
                            src = rel.get("source", "").strip()
                            tgt = rel.get("target", "").strip()
                            if not src or not tgt: continue
                            
                            r_type = re.sub(r'[^a-zA-Z0-9_]', '', rel.get("type", "RELATED").replace(" ", "_").upper()) 
                            if not r_type: r_type = "RELATED"
                            
                            q_rel = f"""
                            MATCH (a:Entity {{id: {json.dumps(src)}}}), (b:Entity {{id: {json.dumps(tgt)}}}) 
                            MERGE (a)-[:{r_type}]->(b)
                            """
                            session.run(q_rel)
                    
                    print(f"      ‚úÖ –§–∞–π–ª {filename} –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –≥—Ä–∞—Ñ.")
                            
                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {e}")

if __name__ == "__main__":
    if not os.path.exists("data"):
        os.makedirs("data")
        print("‚ö†Ô∏è –ü–∞–ø–∫–∞ data –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–Ω–∞ –ø—É—Å—Ç–∞—è.")
    
    try:
        pipeline = HybridGraphPipeline(MEMGRAPH_URI, MEMGRAPH_AUTH)
        pipeline.process_directory("data")
        pipeline.close()
        print("üéâ [6/6] –í—Å–µ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã.")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ–π: {e}")
