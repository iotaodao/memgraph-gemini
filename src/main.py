print("üöÄ [1/6] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Python...")
import os
import json
import uuid
import time
import re
import glob
import sys
from typing import List, Dict, Any

# –õ–æ–≤–∏–º –æ—à–∏–±–∫–∏ –∏–º–ø–æ—Ä—Ç–∞
try:
    from dotenv import load_dotenv
    load_dotenv()
    import google.generativeai as genai
    from chonkie import TokenChunker
    from neo4j import GraphDatabase
    # --- –ù–û–í–û–ï: –ò–º–ø–æ—Ä—Ç Docling –¥–ª—è PDF ---
    from docling.document_converter import DocumentConverter
    print("‚úÖ [2/6] –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (–≤–∫–ª—é—á–∞—è Docling)")
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    sys.exit(1)

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
MEMGRAPH_URI = os.getenv("MEMGRAPH_URI", "bolt://localhost:7687")
MEMGRAPH_AUTH = (os.getenv("MEMGRAPH_USER", ""), os.getenv("MEMGRAPH_PASSWORD", ""))

if "GEMINI_API_KEY" not in os.environ:
    print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω GEMINI_API_KEY")
    sys.exit(1)

genai.configure(api_key=os.environ["GEMINI_API_KEY"])

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
SYSTEM_PROMPT = """
You are an expert Knowledge Graph Engineer.
Extract entities and relationships from the text.

STRICT JSON OUTPUT FORMAT (NO MARKDOWN, NO COMMENTS):
{
  "entities": [
    {"id": "Entity Name", "type": "Category"}
  ],
  "relations": [
    {"source": "Entity Name", "target": "Entity Name", "type": "RELATION_TYPE"}
  ]
}
Normalize IDs. Use SCREAMING_SNAKE_CASE for relation types.
"""

class HybridGraphPipeline:
    def __init__(self, uri, auth, extraction_model="gemini-2.5-flash"):
        print(f"üîå [3/6] –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Memgraph ({uri})...")
        try:
            self.driver = GraphDatabase.driver(uri, auth=auth)
            self.driver.verify_connectivity()
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ!")
        except Exception as e:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ: {e}")
            sys.exit(1)
            
        self.chunker = TokenChunker(tokenizer="gpt2", chunk_size=512, chunk_overlap=50)
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ PDF
        self.pdf_converter = DocumentConverter()
        
        self.extraction_model = genai.GenerativeModel(
            model_name=extraction_model,
            system_instruction=SYSTEM_PROMPT,
            generation_config={"temperature": 0.1, "response_mime_type": "application/json"}
        )
        self.embedding_model_name = "models/text-embedding-004" 

    def close(self):
        self.driver.close()

    def _generate_embedding(self, text: str) -> List[float]:
        try:
            return genai.embed_content(
                model=self.embedding_model_name,
                content=text,
                task_type="retrieval_document"
            )['embedding']
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∞: {e}")
            return []

    def _extract_graph_data(self, text: str) -> Dict[str, Any]:
        try:
            resp = self.extraction_model.generate_content(f"Extract graph from:\\n\\n{text}")
            raw = resp.text
            if "```" in raw:
                raw = re.sub(r"```json|```", "", raw).strip()
            return json.loads(raw)
        except:
            return {"entities": [], "relations": []}

    def _read_file_content(self, filepath: str) -> str:
        """–ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è."""
        ext = os.path.splitext(filepath)[1].lower()
        
        if ext == ".pdf":
            try:
                # Docling –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF –≤ Markdown
                result = self.pdf_converter.convert(filepath)
                return result.document.export_to_markdown()
            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ Docling: {e}")
                return ""
        else:
            # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
            with open(filepath, "r", encoding="utf-8") as f:
                return f.read().replace('\\0', '')

    def process_directory(self, data_dir: str):
        abs_path = os.path.abspath(data_dir)
        print(f"üìÇ [4/6] –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏: {abs_path}")
        
        if not os.path.exists(data_dir):
            print(f"‚ùå –ü–∞–ø–∫–∞ '{data_dir}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            return

        # –î–æ–±–∞–≤–∏–ª–∏ –ø–æ–∏—Å–∫ .pdf
        files = glob.glob(os.path.join(data_dir, "**/*.txt"), recursive=True) + \
                glob.glob(os.path.join(data_dir, "**/*.md"), recursive=True) + \
                glob.glob(os.path.join(data_dir, "**/*.pdf"), recursive=True)
        
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
        if not files: return
        
        print("‚ñ∂Ô∏è [5/6] –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
        with self.driver.session() as session:
            for filepath in files:
                filename = os.path.basename(filepath)
                doc_id = re.sub(r'[^a-zA-Z0-9_-]', '_', filename)
                print(f"   üî™ –ß–∏—Ç–∞—é —Ñ–∞–π–ª: {filename}")
                
                try:
                    # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ
                    text = self._read_file_content(filepath)
                    
                    if not text.strip():
                        print("   ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –ø—Ä–æ—á–∏—Ç–∞–Ω.")
                        continue

                    chunks = self.chunker(text)
                    print(f"      üß© –ß–∞–Ω–∫–æ–≤: {len(chunks)}")

                    # 1. –î–æ–∫—É–º–µ–Ω—Ç
                    session.run(f"MERGE (d:Document {{id: {json.dumps(doc_id)}}})")

                    for i, chunk in enumerate(chunks):
                        graph_data = self._extract_graph_data(chunk.text)
                        vector = self._generate_embedding(chunk.text)
                        chunk_id = str(uuid.uuid4())

                        # 2. –ß–∞–Ω–∫
                        query_chunk = f"""
                        MATCH (d:Document {{id: {json.dumps(doc_id)}}})
                        MERGE (c:Chunk {{id: {json.dumps(chunk_id)}}})
                        SET c.index = {i}
                        SET c.text = {json.dumps(chunk.text)}
                        SET c.embedding = {json.dumps(vector)}
                        MERGE (d)-[:HAS_CHUNK]->(c)
                        """
                        session.run(query_chunk)

                        # 3. –°—É—â–Ω–æ—Å—Ç–∏
                        for ent in graph_data.get("entities", []):
                            e_id = ent.get("id") or ent.get("name")
                            if not e_id: continue
                            e_id = e_id.strip()
                            e_type = re.sub(r'[^a-zA-Z0-9_]', '', ent.get("type", "Thing").strip()) or "Thing"
                            
                            q_ent = f"""
                            MATCH (c:Chunk {{id: {json.dumps(chunk_id)}}}) 
                            MERGE (e:Entity {{id: {json.dumps(e_id)}}}) 
                            ON CREATE SET e.type = {json.dumps(e_type)}
                            MERGE (c)-[:MENTIONS]->(e)
                            """
                            session.run(q_ent)

                        # 4. –°–≤—è–∑–∏
                        for rel in graph_data.get("relations", []):
                            src = rel.get("source", "").strip()
                            tgt = rel.get("target", "").strip()
                            if not src or not tgt: continue
                            
                            r_type = re.sub(r'[^a-zA-Z0-9_]', '', rel.get("type", "RELATED").replace(" ", "_").upper()) or "RELATED"
                            
                            q_rel = f"""
                            MATCH (a:Entity {{id: {json.dumps(src)}}}), (b:Entity {{id: {json.dumps(tgt)}}}) 
                            MERGE (a)-[:{r_type}]->(b)
                            """
                            session.run(q_rel)
                    
                    print(f"      ‚úÖ –§–∞–π–ª {filename} –∑–∞–≥—Ä—É–∂–µ–Ω.")
                            
                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    if not os.path.exists("data"): os.makedirs("data")
    try:
        pipeline = HybridGraphPipeline(MEMGRAPH_URI, MEMGRAPH_AUTH)
        pipeline.process_directory("data")
        pipeline.close()
        print("üéâ [6/6] –ì–æ—Ç–æ–≤–æ.")
    except Exception as e:
        print(f"\n‚ùå –°–±–æ–π: {e}")
